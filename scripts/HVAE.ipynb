{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd073e03da126b73bfff3642ec5261d56fa25c444ea595de51041687efaa60dda41",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "import torch\n",
    "import logging\n",
    "from torch.distributions import kl_divergence\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, Independent\n",
    "from scvi.models.distributions import ZeroInflatedNegativeBinomial, NegativeBinomial\n",
    "from scETM.model import BaseCellModel\n",
    "from scETM.model.model_utils import get_kl, get_fully_connected_layers\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import *\n",
    "from collections import OrderedDict\n",
    "sc.set_figure_params(figsize=(10, 10), fontsize=10, dpi=120, dpi_save=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def gaussian_analytical_kl(mu1, mu2, logsigma1, logsigma2):\n",
    "    return -0.5 + logsigma2 - logsigma1 + 0.5 * (logsigma1.exp() ** 2 + (mu1 - mu2) ** 2) / (logsigma2.exp() ** 2)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def draw_gaussian_diag_samples(mu, logsigma):\n",
    "    eps = torch.empty_like(mu).normal_(0., 1.)\n",
    "    return torch.exp(logsigma) * eps + mu\n",
    "\n",
    "\n",
    "class norm_act_drop(torch.nn.Module):\n",
    "    def __init__(self, size: int, norm_module: str = 'batch', activation: str = 'ReLU', dropout_prob: float = 0.1, final_layer: bool = False):\n",
    "        super().__init__()\n",
    "        self.norm = self.get_norm_layer(size, norm_module) if norm_module != 'none' else None\n",
    "        self.activation, self.dropout = None, None\n",
    "        if not final_layer:\n",
    "            self.activation = getattr(torch.nn, activation)()\n",
    "            self.dropout = torch.nn.Dropout(dropout_prob) if dropout_prob else None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_norm_layer(size, norm_module='none'):\n",
    "        if norm_module == 'none':\n",
    "            return torch.nn.Identity()\n",
    "        elif norm_module == 'batch':\n",
    "            return torch.nn.BatchNorm1d(size)\n",
    "        elif norm_module == 'instance':\n",
    "            return torch.nn.InstanceNorm1d(size)\n",
    "        elif norm_module == 'layer':\n",
    "            return torch.nn.LayerNorm(size)\n",
    "        else:\n",
    "            return NotImplementedError(f\"Not Implemented norm layer {norm_module}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Layer(torch.nn.Module):\n",
    "    def __init__(self, input_channels: int, output_channels: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            norm_act_drop(input_channels),\n",
    "            nn.Linear(input_channels, output_channels, bias = bias)\n",
    "        )\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.bias = bias\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Layer(input_channels={self.input_channels}, output_channels={self.output_channels}, bias={self.bias})'\n",
    "\n",
    "\n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, input_channels: int, output_channels: int, hidden_layers: int, residual: bool = True, pre_norm_act_drop: bool = True):\n",
    "        super().__init__()\n",
    "        self.pre_layer = Layer(input_channels, output_channels) if pre_norm_act_drop else nn.Linear(input_channels, output_channels)\n",
    "        layers = []\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(Layer(output_channels, output_channels))\n",
    "        self.main_layers = nn.Sequential(*layers)\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layer(x)\n",
    "        return (self.main_layers(x) + x) if self.residual else self.main_layers(x)\n",
    "\n",
    "\n",
    "class Block2(torch.nn.Module):\n",
    "    def __init__(self, input_channels: int, output_channels: int, hidden_layers: int, residual: bool = True, pre_norm_act_drop: bool = True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(hidden_layers):\n",
    "            layers.append(nn.Linear(input_channels, input_channels) if i == 0 and not pre_norm_act_drop else Layer(input_channels, input_channels))\n",
    "        self.main_layers = nn.Sequential(*layers)\n",
    "        self.post_layer = Layer(input_channels, output_channels)\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (self.main_layers(x) + x) if self.residual else self.main_layers(x)\n",
    "        return self.post_layer(x)\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_channels: int, hidden_sizes: Sequence[int], hidden_layers: int):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleDict()\n",
    "        for i, size in enumerate(hidden_sizes):\n",
    "            self.blocks[str(size)] = Block(input_channels, size, hidden_layers, pre_norm_act_drop = bool(i))\n",
    "            input_channels = size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hiddens = {}\n",
    "        for size, block in self.blocks.items():\n",
    "            x = block(x)\n",
    "            hiddens[size] = x\n",
    "        return hiddens\n",
    "\n",
    "\n",
    "class DecoderBlock(torch.nn.Module):\n",
    "    def __init__(self, input_channels: int, output_channels: int, hidden_layers: int, pre_norm_act_drop: bool = True):\n",
    "        super().__init__()\n",
    "        self.prior = Block2(input_channels, input_channels * 3, hidden_layers, residual = False, pre_norm_act_drop = pre_norm_act_drop)\n",
    "        self.posterior = Block2(input_channels * 2, input_channels * 2, hidden_layers, residual = False, pre_norm_act_drop = pre_norm_act_drop)\n",
    "        self.output_projection = Block2(input_channels * 2, output_channels, hidden_layers)\n",
    "\n",
    "    def sample_posterior(self, dec_hidden, enc_hidden):\n",
    "        if dec_hidden.shape != enc_hidden.shape:\n",
    "            dec_hidden = dec_hidden.expand(enc_hidden.shape)\n",
    "        qm, qv = self.posterior(torch.cat([dec_hidden, enc_hidden], dim=-1)).chunk(2, dim=-1)\n",
    "        pm, pv, dec_adduct = self.prior(dec_hidden).chunk(3, dim=-1)\n",
    "        qv = qv.clamp(-10, 10)\n",
    "        pv = pv.clamp(-10, 10)\n",
    "        dec_hidden = dec_hidden + dec_adduct\n",
    "        if self.training:\n",
    "            z = draw_gaussian_diag_samples(qm, qv)\n",
    "        else:\n",
    "            z = qm\n",
    "        kl = gaussian_analytical_kl(qm, pm, qv, pv).sum(-1).mean()\n",
    "        return z, dec_hidden, kl\n",
    "\n",
    "    def sample_prior(self, dec_hidden):\n",
    "        pm, pv, dec_adduct = self.prior(dec_hidden).chunk(3, dim=-1)\n",
    "        dec_hidden = dec_hidden + dec_adduct\n",
    "        z = draw_gaussian_diag_samples(pm, pv)\n",
    "        return z, dec_hidden\n",
    "\n",
    "    def forward(self, dec_hidden, enc_hidden):\n",
    "        z, dec_hidden, kl = self.sample_posterior(dec_hidden, enc_hidden)\n",
    "        dec_hidden = self.output_projection(torch.cat([dec_hidden, z], dim=-1))\n",
    "        return z, dec_hidden, kl\n",
    "\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, output_channels: int, hidden_sizes: Sequence[int], hidden_layers: int):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleDict()\n",
    "        hidden_sizes = list(reversed(hidden_sizes)) + [output_channels]\n",
    "        for i, size in enumerate(hidden_sizes[:-1]):\n",
    "            self.blocks[str(size)] = DecoderBlock(size, hidden_sizes[i + 1], hidden_layers, pre_norm_act_drop = bool(i))\n",
    "        self.dec_hidden_init = nn.Parameter(torch.randn((1, hidden_sizes[0])))\n",
    "\n",
    "    def forward(self, enc_hiddens):\n",
    "        dec_hidden = self.dec_hidden_init\n",
    "        zs, kls = OrderedDict(), OrderedDict()\n",
    "        for size, block in self.blocks.items():\n",
    "            zs[size], dec_hidden, kls[size] = block(dec_hidden, enc_hiddens[size])\n",
    "        return zs, dec_hidden, kls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(BaseCellModel):\n",
    "\n",
    "    emb_names = ['z']\n",
    "    clustering_input = 'z'\n",
    "    \n",
    "    def __init__(self, n_genes, n_batches, input_batch_id = False,\n",
    "        hidden_sizes = (256, 128),\n",
    "        hidden_layers = 1,\n",
    "        norm_cells = True,\n",
    "        normed_loss = True,\n",
    "        device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ):\n",
    "        super().__init__(n_genes, n_batches, need_batch = input_batch_id, device = device)\n",
    "\n",
    "        self.norm_cells = norm_cells\n",
    "        self.normed_loss = normed_loss\n",
    "        self.encoder = Encoder(n_genes, hidden_sizes, hidden_layers)\n",
    "        self.decoder = Decoder(n_genes, hidden_sizes, hidden_layers)\n",
    "\n",
    "    def forward(self, data_dict, hyper_param_dict=dict(val=True)):\n",
    "        cells, library_size = data_dict['cells'], data_dict['library_size']\n",
    "        normed_cells = cells / library_size\n",
    "        input_cells = normed_cells if self.norm_cells else cells\n",
    "        # if self.input_batch_id:\n",
    "        #     input_cells = torch.cat((input_cells, self._get_batch_indices_oh(data_dict)), dim=1)\n",
    "        \n",
    "        hiddens = self.encoder(input_cells)\n",
    "        zs, recon_logit, kls = self.decoder(hiddens)\n",
    "        recon_log = F.log_softmax(recon_logit, dim=-1)\n",
    "        nll = (-recon_log * normed_cells if self.normed_loss else cells).sum(-1).mean()\n",
    "\n",
    "        fwd_dict = dict(\n",
    "            z = torch.cat(list(zs.values()), dim=-1),\n",
    "            recon_log=recon_log,\n",
    "            nll = nll\n",
    "        )\n",
    "        fwd_dict.update(zs)\n",
    "\n",
    "        if not self.training:\n",
    "            return fwd_dict\n",
    "\n",
    "        total_kl = 0.\n",
    "        for kl in kls.values():\n",
    "            total_kl += kl\n",
    "        loss = nll + hyper_param_dict['beta'] * total_kl\n",
    "\n",
    "        record = dict(loss=loss, nll=nll, total_kl=total_kl)\n",
    "        record = {k: v.detach().item() for k, v in record.items()}\n",
    "        return loss, fwd_dict, record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2021-04-28 20:13:31,062] INFO - scETM.logging_utils: UnsupervisedTrainer.__init__(Model(\n",
      "  (encoder): Encoder(\n",
      "    (blocks): ModuleDict(\n",
      "      (128): Block(\n",
      "        (pre_layer): Linear(in_features=22964, out_features=128, bias=True)\n",
      "        (main_layers): Sequential(\n",
      "          (0): Layer(input_channels=128, output_channels=128, bias=True)\n",
      "          (1): Layer(input_channels=128, output_channels=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (64): Block(\n",
      "        (pre_layer): Layer(input_channels=128, output_channels=64, bias=True)\n",
      "        (main_layers): Sequential(\n",
      "          (0): Layer(input_channels=64, output_channels=64, bias=True)\n",
      "          (1): Layer(input_channels=64, output_channels=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (32): Block(\n",
      "        (pre_layer): Layer(input_channels=64, output_channels=32, bias=True)\n",
      "        (main_layers): Sequential(\n",
      "          (0): Layer(input_channels=32, output_channels=32, bias=True)\n",
      "          (1): Layer(input_channels=32, output_channels=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (blocks): ModuleDict(\n",
      "      (32): DecoderBlock(\n",
      "        (prior): Block2(\n",
      "          (main_layers): Sequential(\n",
      "            (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (1): Layer(input_channels=32, output_channels=32, bias=True)\n",
      "          )\n",
      "          (post_layer): Layer(input_channels=32, output_channels=96, bias=True)\n",
      "        )\n",
      "        (posterior): Block2(\n",
      "          (main_layers): Sequential(\n",
      "            (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (1): Layer(input_channels=64, output_channels=64, bias=True)\n",
      "          )\n",
      "          (post_layer): Layer(input_channels=64, output_channels=64, bias=True)\n",
      "        )\n",
      "        (output_projection): Block2(\n",
      "          (main_layers): Sequential(\n",
      "            (0): Layer(input_channels=64, output_channels=64, bias=True)\n",
      "            (1): Layer(input_channels=64, output_channels=64, bias=True)\n",
      "          )\n",
      "          (post_layer): Layer(input_channels=64, output_channels=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (64): DecoderBlock(\n",
      "        (prior): Block2(\n",
      "          (main_layers): Sequential(\n",
      "            (0): Layer(input_channels=64, output_channels=64, bias=True)\n",
      "            (1): Layer(input_channels=64, output_channels=64, bias=True)\n",
      "          )\n",
      "          (post_layer): Layer(input_channels=64, output_channels=192, bias=True)\n",
      "        )\n",
      "        (posterior): Block2(\n",
      "          (main_layers): Sequential(\n",
      "            (0): Layer(input_channels=128, output_channels=128, bias=True)\n",
      "            (1): Layer(input_channels=128, output_channels=128, bias=True)\n",
      "          )\n",
      "          (post_layer): Layer(input_channels=128, output_channels=128, bias=True)\n",
      "        )\n",
      "        (output_projection): Block2(\n",
      "          (main_layers): Sequential(\n",
      "            (0): Layer(input_channels=128, output_channels=128, bias=True)\n",
      "            (1): Layer(input_channels=128, output_channels=128, bias=True)\n",
      "          )\n",
      "          (post_layer): Layer(input_channels=128, output_channels=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (128): DecoderBlock(\n",
      "        (prior): Block2(\n",
      "          (main_layers): Sequential(\n",
      "            (0): Layer(input_channels=128, output_channels=128, bias=True)\n",
      "            (1): Layer(input_channels=128, output_channels=128, bias=True)\n",
      "          )\n",
      "          (post_layer): Layer(input_channels=128, output_channels=384, bias=True)\n",
      "        )\n",
      "        (posterior): Block2(\n",
      "          (main_layers): Sequential(\n",
      "            (0): Layer(input_channels=256, output_channels=256, bias=True)\n",
      "            (1): Layer(input_channels=256, output_channels=256, bias=True)\n",
      "          )\n",
      "          (post_layer): Layer(input_channels=256, output_channels=256, bias=True)\n",
      "        )\n",
      "        (output_projection): Block2(\n",
      "          (main_layers): Sequential(\n",
      "            (0): Layer(input_channels=256, output_channels=256, bias=True)\n",
      "            (1): Layer(input_channels=256, output_channels=256, bias=True)\n",
      "          )\n",
      "          (post_layer): Layer(input_channels=256, output_channels=22964, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "), AnnData object with n_obs × n_vars = 44949 × 22964\n",
      "    obs: 'tissue_new', 'Neurog3>0_raw', 'Neurog3>0_scaled', 'cell_ontology_class', 'cell_ontology_id', 'cluster.ids', 'free_annotation', 'mouse.id', 'mouse.sex', 'plate.barcode', 'subsetA', 'subsetA_cluster.ids', 'subsetB', 'subsetB_cluster.ids', 'subsetC', 'subsetC_cluster.ids', 'subsetD', 'subsetD_cluster.ids', 'subsetE', 'subsetE_cluster.ids', 'subtissue', 'tissue', 'tissue_tSNE_1', 'tissue_tSNE_2', 'ercc_perc', 'ribo_perc', 'rn45s_perc', 'n_genes', 'n_counts', 'cell_types', 'batch_indices'\n",
      "    var: 'n_counts', ckpt_dir = ../results/HVAE, init_lr = 0.005, lr_decay = 6e-05, batch_size = 4000, train_instance_name = TM_1e-9)\n",
      "[2021-04-28 20:13:31,065] INFO - scETM.trainer: ckpt_dir: ../results/HVAE\\TM_1e-9_04_28-20_13_31\n",
      "[2021-04-28 20:13:31,067] INFO - scETM.logging_utils: UnsupervisedTrainer.train(<scETM.trainer.UnsupervisedTrainer object at 0x0000019744CD76C8>, n_epochs = 4000, eval_every = 1000, max_kl_weight = 1e-09, eval_kwargs = {'resolutions': (0.24, 0.32, 0.48, 0.64, 0.8, 1, 1.3)})\n",
      "[2021-04-28 20:13:34,547] INFO - scETM.trainer: ==========Epoch 0==========\n",
      "[2021-04-28 20:13:34,548] INFO - scETM.trainer: pmem(rss=7111163904, vms=13679394816, num_page_faults=2340819, peak_wset=7113469952, wset=7111163904, peak_paged_pool=4964920, paged_pool=4964872, peak_nonpaged_pool=1812384, nonpaged_pool=411072, pagefile=13679394816, peak_pagefile=13682298880, private=13679394816)\n",
      "[2021-04-28 20:13:34,549] INFO - scETM.trainer: lr          : 0.00499970000899982\n",
      "[2021-04-28 20:13:34,550] INFO - scETM.trainer: kl_weight   :       0.0000\n",
      "[2021-04-28 20:13:34,550] INFO - scETM.trainer: loss        :      10.0778\n",
      "[2021-04-28 20:13:34,551] INFO - scETM.trainer: nll         :      10.0778\n",
      "[2021-04-28 20:13:34,552] INFO - scETM.trainer: total_kl    : 77375983321088.0000\n",
      "[2021-04-28 20:13:34,553] INFO - scETM.trainer: max_norm    :      50.6124\n",
      "[2021-04-28 20:13:38,083] INFO - scETM.logging_utils: evaluate(adata = AnnData object with n_obs × n_vars = 44949 × 22964\n",
      "    obs: 'tissue_new', 'Neurog3>0_raw', 'Neurog3>0_scaled', 'cell_ontology_class', 'cell_ontology_id', 'cluster.ids', 'free_annotation', 'mouse.id', 'mouse.sex', 'plate.barcode', 'subsetA', 'subsetA_cluster.ids', 'subsetB', 'subsetB_cluster.ids', 'subsetC', 'subsetC_cluster.ids', 'subsetD', 'subsetD_cluster.ids', 'subsetE', 'subsetE_cluster.ids', 'subtissue', 'tissue', 'tissue_tSNE_1', 'tissue_tSNE_2', 'ercc_perc', 'ribo_perc', 'rn45s_perc', 'n_genes', 'n_counts', 'cell_types', 'batch_indices'\n",
      "    var: 'n_counts'\n",
      "    obsm: 'z', embedding_key = z, batch_col = batch_indices, plot_fname = TM_1e-9_z_epoch0, plot_dir = ../results/HVAE\\TM_1e-9_04_28-20_13_31, resolutions = (0.24, 0.32, 0.48, 0.64, 0.8, 1, 1.3))\n",
      "[2021-04-28 20:13:55,789] INFO - scETM.eval_utils: Performing leiden clustering\n",
      "[2021-04-28 20:14:15,763] INFO - scETM.eval_utils: Resolution:  0.24\tARI:  0.4690\tNMI:  0.7025\tbARI:  0.0169\t# labels: 23\n",
      "[2021-04-28 20:14:54,997] INFO - scETM.eval_utils: Resolution:  0.32\tARI:  0.4844\tNMI:  0.7198\tbARI:  0.0143\t# labels: 29\n",
      "[2021-04-28 20:15:36,586] INFO - scETM.eval_utils: Resolution:  0.48\tARI:  0.4927\tNMI:  0.7055\tbARI:  0.0199\t# labels: 36\n",
      "[2021-04-28 20:15:57,165] INFO - scETM.eval_utils: Resolution:  0.64\tARI:  0.5001\tNMI:  0.7117\tbARI:  0.0203\t# labels: 40\n",
      "[2021-04-28 20:16:39,032] INFO - scETM.eval_utils: Resolution:   0.8\tARI:  0.4742\tNMI:  0.7113\tbARI:  0.0202\t# labels: 44\n",
      "[2021-04-28 20:17:20,797] INFO - scETM.eval_utils: Resolution:     1\tARI:  0.4417\tNMI:  0.7022\tbARI:  0.0208\t# labels: 53\n",
      "[2021-04-28 20:17:54,385] INFO - scETM.eval_utils: Resolution:   1.3\tARI:  0.4138\tNMI:  0.7032\tbARI:  0.0182\t# labels: 59\n",
      "[2021-04-28 20:17:54,388] INFO - scETM.eval_utils: Calculating batch mixing entropy...\n",
      "[2021-04-28 20:18:04,236] INFO - scETM.eval_utils: z_BE:  1.0429\n",
      "[2021-04-28 20:18:04,238] INFO - scETM.eval_utils: Calculating kbet...\n",
      "[2021-04-28 20:18:15,369] INFO - scETM.eval_utils: z_kBET:  0.3189\n",
      "[2021-04-28 20:18:58,106] INFO - scETM.trainer: ==========End of evaluation==========\n",
      "[2021-04-28 21:11:26,482] INFO - scETM.trainer: ==========Epoch 1000==========\n",
      "[2021-04-28 21:11:26,483] INFO - scETM.trainer: pmem(rss=5869797376, vms=16946036736, num_page_faults=1288524376, peak_wset=9525256192, wset=5869797376, peak_paged_pool=5291400, paged_pool=5267464, peak_nonpaged_pool=1812384, nonpaged_pool=446080, pagefile=16946036736, peak_pagefile=17979654144, private=16946036736)\n",
      "[2021-04-28 21:11:26,483] INFO - scETM.trainer: lr          : 0.0025476157261772386\n",
      "[2021-04-28 21:11:26,484] INFO - scETM.trainer: kl_weight   :       0.0000\n",
      "[2021-04-28 21:11:26,486] INFO - scETM.trainer: loss        :       6.5220\n",
      "[2021-04-28 21:11:26,487] INFO - scETM.trainer: nll         :       6.5111\n",
      "[2021-04-28 21:11:26,489] INFO - scETM.trainer: total_kl    : 79487732009.9473\n",
      "[2021-04-28 21:11:26,492] INFO - scETM.trainer: max_norm    :       2.5143\n",
      "[2021-04-28 21:11:29,645] INFO - scETM.logging_utils: evaluate(adata = AnnData object with n_obs × n_vars = 44949 × 22964\n",
      "    obs: 'tissue_new', 'Neurog3>0_raw', 'Neurog3>0_scaled', 'cell_ontology_class', 'cell_ontology_id', 'cluster.ids', 'free_annotation', 'mouse.id', 'mouse.sex', 'plate.barcode', 'subsetA', 'subsetA_cluster.ids', 'subsetB', 'subsetB_cluster.ids', 'subsetC', 'subsetC_cluster.ids', 'subsetD', 'subsetD_cluster.ids', 'subsetE', 'subsetE_cluster.ids', 'subtissue', 'tissue', 'tissue_tSNE_1', 'tissue_tSNE_2', 'ercc_perc', 'ribo_perc', 'rn45s_perc', 'n_genes', 'n_counts', 'cell_types', 'batch_indices', 'leiden_0.24', 'leiden_0.32', 'leiden_0.48', 'leiden_0.64', 'leiden_0.8', 'leiden_1', 'leiden_1.3'\n",
      "    var: 'n_counts'\n",
      "    uns: 'neighbors', 'leiden', 'umap', 'leiden_0.64_colors', 'batch_indices_colors', 'cell_types_colors'\n",
      "    obsm: 'z', 'knn_indices', 'X_umap'\n",
      "    obsp: 'distances', 'connectivities', embedding_key = z, batch_col = batch_indices, plot_fname = TM_1e-9_z_epoch1000, plot_dir = ../results/HVAE\\TM_1e-9_04_28-20_13_31, resolutions = (0.24, 0.32, 0.48, 0.64, 0.8, 1, 1.3))\n",
      "[2021-04-28 21:11:38,686] INFO - scETM.eval_utils: Performing leiden clustering\n",
      "[2021-04-28 21:12:23,838] INFO - scETM.eval_utils: Resolution:  0.24\tARI:  0.3339\tNMI:  0.5773\tbARI:  0.0063\t# labels: 17\n",
      "[2021-04-28 21:12:49,363] INFO - scETM.eval_utils: Resolution:  0.32\tARI:  0.3770\tNMI:  0.5885\tbARI:  0.0078\t# labels: 19\n",
      "[2021-04-28 21:13:23,463] INFO - scETM.eval_utils: Resolution:  0.48\tARI:  0.4568\tNMI:  0.6274\tbARI:  0.0110\t# labels: 25\n",
      "[2021-04-28 21:13:53,135] INFO - scETM.eval_utils: Resolution:  0.64\tARI:  0.4564\tNMI:  0.6348\tbARI:  0.0113\t# labels: 31\n",
      "[2021-04-28 21:14:25,255] INFO - scETM.eval_utils: Resolution:   0.8\tARI:  0.4578\tNMI:  0.6402\tbARI:  0.0115\t# labels: 34\n",
      "[2021-04-28 21:15:12,192] INFO - scETM.eval_utils: Resolution:     1\tARI:  0.4306\tNMI:  0.6441\tbARI:  0.0124\t# labels: 42\n",
      "[2021-04-28 21:16:03,917] INFO - scETM.eval_utils: Resolution:   1.3\tARI:  0.4348\tNMI:  0.6563\tbARI:  0.0120\t# labels: 52\n",
      "[2021-04-28 21:16:03,920] INFO - scETM.eval_utils: Calculating batch mixing entropy...\n",
      "[2021-04-28 21:16:15,262] INFO - scETM.eval_utils: z_BE:  0.9807\n",
      "[2021-04-28 21:16:15,264] INFO - scETM.eval_utils: Calculating kbet...\n",
      "[2021-04-28 21:16:27,247] INFO - scETM.eval_utils: z_kBET:  0.2678\n",
      "[2021-04-28 21:17:02,641] INFO - scETM.trainer: ==========End of evaluation==========\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5da5781172e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mtrain_instance_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"{dataset_name}_1e-9\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m )\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_kl_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolutions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresolutions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\scETM\\logging_utils.py\u001b[0m in \u001b[0;36mlog_arguments_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0margs_kwargs_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0margs_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs_str\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{name}({args_kwargs_str})'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlog_arguments_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\scETM\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, n_epochs, eval_every, n_samplers, kl_warmup_ratio, min_kl_weight, max_kl_weight, eval, batch_col, save_model_ckpt, record_log_path, eval_result_log_path, eval_kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[1;31m# train for one step, record tracked items (e.g. loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m             \u001b[0mnew_record\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhyper_param_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m             \u001b[0mrecorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_record\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_ckpt_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\scETM\\model\\BaseCellModel.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, optimizer, data_dict, hyper_param_dict)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mnorms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mnew_record\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_norm'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnorms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_record\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                    group['eps'])\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scETM import UnsupervisedTrainer, evaluate\n",
    "adata = anndata.read_h5ad(\"../../../data/TM/FACS.h5ad\")\n",
    "adata.obs['cell_types'] = adata.obs.cell_ontology_class\n",
    "adata.obs['batch_indices'] = adata.obs['mouse.id']\n",
    "dataset_name = 'TM'\n",
    "resolutions = (0.24, 0.32, 0.48, 0.64, 0.8, 1, 1.3)\n",
    "# dataset_name = 'cortex'\n",
    "# adata = anndata.read_h5ad(\"../../../data/cortex/cortex_full.h5ad\")\n",
    "# resolutions = (0.08, 0.12, 0.16, 0.24, 0.32, 0.48, 0.64)\n",
    "model = Model(adata.n_vars, adata.obs.batch_indices.nunique(), hidden_sizes = (128, 64, 32), hidden_layers = 2).to(torch.device('cuda'))\n",
    "trainer = UnsupervisedTrainer(\n",
    "        model,\n",
    "        adata,\n",
    "        ckpt_dir = '../results/HVAE',\n",
    "        init_lr = 5e-3,\n",
    "        lr_decay = 6e-5,\n",
    "        batch_size = 4000,\n",
    "        train_instance_name = f\"{dataset_name}_1e-9\"\n",
    ")\n",
    "trainer.train(n_epochs = 4000, eval_every = 1000, max_kl_weight=1e-9, eval_kwargs = dict(resolutions = resolutions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_cell_embeddings_and_nll(adata, batch_size = 4000, emb_names=['z', '128', '64', '32'])\n",
    "from scETM import evaluate\n",
    "evaluate(adata, embedding_key = '128', plot_dir = trainer.ckpt_dir, plot_fname = f'cortex_z_epoch{int(trainer.epoch)}_128', resolutions = resolutions, n_jobs = 1)\n",
    "evaluate(adata, embedding_key = '64', plot_dir = trainer.ckpt_dir, plot_fname = f'cortex_z_epoch{int(trainer.epoch)}_64', resolutions = resolutions, n_jobs = 1)\n",
    "evaluate(adata, embedding_key = '32', plot_dir = trainer.ckpt_dir, plot_fname = f'cortex_z_epoch{int(trainer.epoch)}_32', resolutions = resolutions, n_jobs = 1)"
   ]
  }
 ]
}